{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_DEFAULTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bcf86c4851b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Copy the default model configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmdl_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_DEFAULTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Convolution network settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_DEFAULTS' is not defined"
     ]
    }
   ],
   "source": [
    "# ================= Configure the model =================\n",
    "\n",
    "# Copy the default model configuration\n",
    "mdl_cfg = MODEL_DEFAULTS.copy()                # eg. appo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "# Convolution network settings\n",
    "mdl_cfg[\"conv_filters\"] = None                  # Filter config. List of [out_channels, kernel, stride] for each filter\n",
    "mdl_cfg[\"conv_activation\"] = \"relu\"             # Nonlinearity for built-in convnet\n",
    "\n",
    "# Fully-connected network settings\n",
    "mdl_cfg[\"fcnet_activation\"] = \"tanh\"            # Nonlinearity for built-in fully connected net (tanh, relu)\n",
    "mdl_cfg[\"fcnet_hiddens\"] = [64, 64]             # Number of hidden layers for fully connected net\n",
    "mdl_cfg[\"no_final_linear\"] = False              # Whether to skip the final linear layer used to resize the outputs to `num_outputs`\n",
    "mdl_cfg[\"free_log_std\"] = True                  # The last half of the output layer does not dependent on the input\n",
    "mdl_cfg[\"vf_share_layers\"] = True               # Whether layers should be shared for the value function.\n",
    "\n",
    "# LTSM network settings\n",
    "mdl_cfg[\"use_lstm\"] = False                     # Whether to wrap the model with a LSTM\n",
    "mdl_cfg[\"max_seq_len\"] = 20                     # Max seq len for training the LSTM\n",
    "mdl_cfg[\"lstm_cell_size\"] = 256                 # Size of the LSTM cell\n",
    "mdl_cfg[\"lstm_use_prev_action_reward\"] = False  # Whether to feed a_{t-1}, r_{t-1} to LSTM\n",
    "\n",
    "# Custom model settings\n",
    "mdl_cfg[\"custom_model\"] = None # Name of a custom model to use\n",
    "mdl_cfg[\"custom_options\"] = {} # Dict of extra options to pass to the custom models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Configure rllib =================\n",
    "\n",
    "# Copy the default rllib configuration\n",
    "rllib_cfg = COMMON_CONFIG.copy()\n",
    "\n",
    "# Ressources settings\n",
    "rllib_cfg[\"use_pytorch\"] = True        # Use PyTorch instead of Tensorflow\n",
    "rllib_cfg[\"num_gpus\"] = 1              # Number of GPUs to reserve for the trainer process\n",
    "rllib_cfg[\"num_workers\"] = 8           # Number of rollout worker actors for parallel sampling\n",
    "rllib_cfg[\"num_envs_per_worker\"] = 16  # Number of environments per worker\n",
    "rllib_cfg[\"num_cpus_per_worker\"] = 1   # Number of CPUs to reserve per worker\n",
    "rllib_cfg[\"num_cpus_for_driver\"] = 0   # Number of CPUs to allocate for the trainer\n",
    "\n",
    "# Rollout settings\n",
    "rllib_cfg[\"rollout_fragment_length\"] = 32      # Sample batches of this size (mult. by `num_envs_per_worker`) are collected from rollout workers\n",
    "rllib_cfg[\"train_batch_size\"] = 512            # Sample batches are concatenated together into batches of this size\n",
    "rllib_cfg[\"batch_mode\"] = \"complete_episodes\"  # Whether to rollout \"complete_episodes\" or \"truncate_episodes\" to `rollout_fragment_length`\n",
    "rllib_cfg[\"sample_async\"] = False              # Use a background thread for sampling (slightly off-policy)\n",
    "rllib_cfg[\"observation_filter\"] = \"NoFilter\"   # Element-wise observation filter [\"NoFilter\", \"MeanStdFilter\"]\n",
    "rllib_cfg[\"metrics_smoothing_episodes\"] = 100  # Smooth metrics over this many episodes\n",
    "rllib_cfg[\"seed\"] = None                       # sets the random seed of each worker (in conjunction with worker_index)\n",
    "\n",
    "# Environment settings\n",
    "rllib_cfg[\"horizon\"] = None             # Number of steps after which the episode is forced to terminate\n",
    "rllib_cfg[\"soft_horizon\"] = True        # Calculate rewards but don't reset the environment when the horizon is hit\n",
    "rllib_cfg[\"no_done_at_end\"] = True      # Don't set 'done' at the end of the episode\n",
    "rllib_cfg[\"env_config\"] = {}            # Arguments to pass to the env creator\n",
    "rllib_cfg[\"normalize_actions\"] = False  # Normalize actions to the upper and lower bounds of the action space\n",
    "rllib_cfg[\"clip_actions\"] = False       # Whether to clip actions to the upper and lower bounds of the action space\n",
    "\n",
    "# Learning settings\n",
    "rllib_cfg[\"gamma\"] = 0.99             # Discount factor of the MDP\n",
    "rllib_cfg[\"lr\"] = 1.0e-3              # Learning rate\n",
    "rllib_cfg[\"shuffle_buffer_size\"] = 0  # Shuffle input batches via a sliding window buffer of this size (0 = disable)\n",
    "rllib_cfg[\"log_level\"] = \"WARN\"       # Set the ray.rllib.* log level for the agent process and its workers [DEBUG, INFO, WARN, or ERROR]\n",
    "rllib_cfg[\"model\"] = mdl_cfg          # Policy model configuration\n",
    "\n",
    "# ================= Configure the learning algorithm ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Configure the learning algorithm =================\n",
    "\n",
    "# Select PPO algorithm\n",
    "from ray.rllib.agents.ppo import PPOTrainer as Trainer, DEFAULT_CONFIG\n",
    "  \n",
    "# Copy the default learning algorithm configuration, including PPO-specific parameters,\n",
    "# then overwrite the common parameters that has been updated ONLY.\n",
    "agent_cfg = DEFAULT_CONFIG\n",
    "for key, value in rllib_cfg.items():\n",
    "    if COMMON_CONFIG[key] != value:\n",
    "        agent_cfg[key] = value\n",
    "\n",
    "# Optimizer settings\n",
    "agent_cfg[\"sgd_minibatch_size\"] = 128  # Total SGD batch size across all devices for SGD. This defines the minibatch size of each SGD epoch\n",
    "agent_cfg[\"num_sgd_iter\"] = 4          # Number of SGD epochs to execute per train batch\n",
    "agent_cfg[\"shuffle_sequences\"] = True  # Whether to shuffle sequences in the batch when training\n",
    "\n",
    "# Estimators settings\n",
    "agent_cfg[\"use_gae\"] = True      # Use the Generalized Advantage Estimator (GAE) with a value function (https://arxiv.org/pdf/1506.02438.pdf)\n",
    "agent_cfg[\"use_critic\"] = False  # Use a critic as a value baseline (otherwise don't use any; required for using GAE).\n",
    "agent_cfg[\"lambda\"] = 0.95       # The GAE(lambda) parameter.\n",
    "\n",
    "# Learning and optimization settings\n",
    "agent_cfg[\"lr_schedule\"] = None             # Learning rate schedule\n",
    "agent_cfg[\"kl_coeff\"] = 0.2                 # Initial coefficient for KL divergence\n",
    "agent_cfg[\"kl_target\"] = 0.01               # Target value for KL divergence\n",
    "agent_cfg[\"vf_share_layers\"] = False        # Share layers for value function. If you set this to True, it's important to tune vf_loss_coeff\n",
    "agent_cfg[\"vf_loss_coeff\"] = 0.5            # Coefficient of the value function loss\n",
    "agent_cfg[\"entropy_coeff\"] = 0.01           # Coefficient of the entropy regularizer\n",
    "agent_cfg[\"entropy_coeff_schedule\"] = None  # Decay schedule for the entropy regularizer\n",
    "agent_cfg[\"clip_param\"] = 0.2               # PPO clip parameter\n",
    "agent_cfg[\"vf_clip_param\"] = float(\"inf\")   # Clip param for the value function. Note that this is sensitive to the scale of the rewards (-1 to disable)\n",
    "agent_cfg[\"grad_clip\"] = None               # Clip the global norm of gradients by this amount (None = disable) (No working with PyTorch ML backend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
