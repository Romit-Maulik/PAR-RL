{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================= Configure the model =================\n\n# Copy the default model configuration\nmdl_cfg = MODEL_DEFAULTS.copy()\n\n# Convolution network settings\nmdl_cfg[\"conv_filters\"] = None                  # Filter config. List of [out_channels, kernel, stride] for each filter\nmdl_cfg[\"conv_activation\"] = \"relu\"             # Nonlinearity for built-in convnet\n\n# Fully-connected network settings\nmdl_cfg[\"fcnet_activation\"] = \"tanh\"            # Nonlinearity for built-in fully connected net (tanh, relu)\nmdl_cfg[\"fcnet_hiddens\"] = [64, 64]             # Number of hidden layers for fully connected net\nmdl_cfg[\"no_final_linear\"] = False              # Whether to skip the final linear layer used to resize the outputs to `num_outputs`\nmdl_cfg[\"free_log_std\"] = True                  # The last half of the output layer does not dependent on the input\nmdl_cfg[\"vf_share_layers\"] = True               # Whether layers should be shared for the value function.\n\n# LTSM network settings\nmdl_cfg[\"use_lstm\"] = False                     # Whether to wrap the model with a LSTM\nmdl_cfg[\"max_seq_len\"] = 20                     # Max seq len for training the LSTM\nmdl_cfg[\"lstm_cell_size\"] = 256                 # Size of the LSTM cell\nmdl_cfg[\"lstm_use_prev_action_reward\"] = False  # Whether to feed a_{t-1}, r_{t-1} to LSTM\n\n# Custom model settings\nmdl_cfg[\"custom_model\"] = None # Name of a custom model to use\nmdl_cfg[\"custom_options\"] = {} # Dict of extra options to pass to the custom models\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================= Configure rllib =================\n\n# Copy the default rllib configuration\nrllib_cfg = COMMON_CONFIG.copy()\n\n# Ressources settings\nrllib_cfg[\"use_pytorch\"] = True        # Use PyTorch instead of Tensorflow\nrllib_cfg[\"num_gpus\"] = 1              # Number of GPUs to reserve for the trainer process\nrllib_cfg[\"num_workers\"] = 8           # Number of rollout worker actors for parallel sampling\nrllib_cfg[\"num_envs_per_worker\"] = 16  # Number of environments per worker\nrllib_cfg[\"num_cpus_per_worker\"] = 1   # Number of CPUs to reserve per worker\nrllib_cfg[\"num_cpus_for_driver\"] = 0   # Number of CPUs to allocate for the trainer\n\n# Rollout settings\nrllib_cfg[\"rollout_fragment_length\"] = 32      # Sample batches of this size (mult. by `num_envs_per_worker`) are collected from rollout workers\nrllib_cfg[\"train_batch_size\"] = 512            # Sample batches are concatenated together into batches of this size\nrllib_cfg[\"batch_mode\"] = \"complete_episodes\"  # Whether to rollout \"complete_episodes\" or \"truncate_episodes\" to `rollout_fragment_length`\nrllib_cfg[\"sample_async\"] = False              # Use a background thread for sampling (slightly off-policy)\nrllib_cfg[\"observation_filter\"] = \"NoFilter\"   # Element-wise observation filter [\"NoFilter\", \"MeanStdFilter\"]\nrllib_cfg[\"metrics_smoothing_episodes\"] = 100  # Smooth metrics over this many episodes\nrllib_cfg[\"seed\"] = None                       # sets the random seed of each worker (in conjunction with worker_index)\n\n# Environment settings\nrllib_cfg[\"horizon\"] = None             # Number of steps after which the episode is forced to terminate\nrllib_cfg[\"soft_horizon\"] = True        # Calculate rewards but don't reset the environment when the horizon is hit\nrllib_cfg[\"no_done_at_end\"] = True      # Don't set 'done' at the end of the episode\nrllib_cfg[\"env_config\"] = {}            # Arguments to pass to the env creator\nrllib_cfg[\"normalize_actions\"] = False  # Normalize actions to the upper and lower bounds of the action space\nrllib_cfg[\"clip_actions\"] = False       # Whether to clip actions to the upper and lower bounds of the action space\n\n# Learning settings\nrllib_cfg[\"gamma\"] = 0.99             # Discount factor of the MDP\nrllib_cfg[\"lr\"] = 1.0e-3              # Learning rate\nrllib_cfg[\"shuffle_buffer_size\"] = 0  # Shuffle input batches via a sliding window buffer of this size (0 = disable)\nrllib_cfg[\"log_level\"] = \"WARN\"       # Set the ray.rllib.* log level for the agent process and its workers [DEBUG, INFO, WARN, or ERROR]\nrllib_cfg[\"model\"] = mdl_cfg          # Policy model configuration\n\n# ================= Configure the learning algorithm =================","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================= Configure the learning algorithm =================\n\n# Select PPO algorithm\nfrom ray.rllib.agents.ppo import PPOTrainer as Trainer, DEFAULT_CONFIG\n  \n# Copy the default learning algorithm configuration, including PPO-specific parameters,\n# then overwrite the common parameters that has been updated ONLY.\nagent_cfg = DEFAULT_CONFIG\nfor key, value in rllib_cfg.items():\n    if COMMON_CONFIG[key] != value:\n        agent_cfg[key] = value\n\n# Optimizer settings\nagent_cfg[\"sgd_minibatch_size\"] = 128  # Total SGD batch size across all devices for SGD. This defines the minibatch size of each SGD epoch\nagent_cfg[\"num_sgd_iter\"] = 4          # Number of SGD epochs to execute per train batch\nagent_cfg[\"shuffle_sequences\"] = True  # Whether to shuffle sequences in the batch when training\n\n# Estimators settings\nagent_cfg[\"use_gae\"] = True      # Use the Generalized Advantage Estimator (GAE) with a value function (https://arxiv.org/pdf/1506.02438.pdf)\nagent_cfg[\"use_critic\"] = False  # Use a critic as a value baseline (otherwise don't use any; required for using GAE).\nagent_cfg[\"lambda\"] = 0.95       # The GAE(lambda) parameter.\n\n# Learning and optimization settings\nagent_cfg[\"lr_schedule\"] = None             # Learning rate schedule\nagent_cfg[\"kl_coeff\"] = 0.2                 # Initial coefficient for KL divergence\nagent_cfg[\"kl_target\"] = 0.01               # Target value for KL divergence\nagent_cfg[\"vf_share_layers\"] = False        # Share layers for value function. If you set this to True, it's important to tune vf_loss_coeff\nagent_cfg[\"vf_loss_coeff\"] = 0.5            # Coefficient of the value function loss\nagent_cfg[\"entropy_coeff\"] = 0.01           # Coefficient of the entropy regularizer\nagent_cfg[\"entropy_coeff_schedule\"] = None  # Decay schedule for the entropy regularizer\nagent_cfg[\"clip_param\"] = 0.2               # PPO clip parameter\nagent_cfg[\"vf_clip_param\"] = float(\"inf\")   # Clip param for the value function. Note that this is sensitive to the scale of the rewards (-1 to disable)\nagent_cfg[\"grad_clip\"] = None               # Clip the global norm of gradients by this amount (None = disable) (No working with PyTorch ML backend)","metadata":{},"execution_count":null,"outputs":[]}]}